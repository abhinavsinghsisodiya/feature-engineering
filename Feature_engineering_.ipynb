{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering"
      ],
      "metadata": {
        "id": "jLwVAUz2UT2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter\n",
        "-; A parameter is a number describing a whole population (e.g., population mean), while a statistic is a number describing a sample (e.g., sample mean). The goal of quantitative research is to understand characteristics of populations by finding parameters.\n",
        "\n",
        "\n",
        "\n",
        "2.What is correlation?\n",
        "What does negative correlation mean?\n",
        "-; Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.\n",
        "A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable decreases, and vice versa. This is a relationship where the variables move in opposite directions.\n",
        "\n",
        "\n",
        "\n",
        "3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "-;Machine Learning (ML) is a field of artificial intelligence that enables computer systems to learn from data without being explicitly programmed. It involves algorithms that can find patterns and make predictions based on that data. Key components include algorithms, data, models, and predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "-; A lower loss value generally indicates a better model, as it means the model's predictions are closer to the actual values. Loss functions quantify the error between a model's predictions and the ground truth, and the goal during training is to minimize this error. A model with a consistently low loss value on both training and validation data is more likely to generalize well to new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. What are continuous and categorical variables?\n",
        "-;In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups. Continuous variables can be measured and have an infinite number of possible values, while categorical variables are qualitative and limited to a specific set of labels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?\n",
        "-;To handle categorical variables in machine learning, common techniques include label encoding, one-hot encoding, ordinal encoding, and frequency/count encoding. Label encoding assigns numerical values to categories, while one-hot encoding creates binary columns for each category. Ordinal encoding preserves the order of categories, and frequency encoding replaces categories with their frequencies.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "-; In the context of machine learning, \"training\" and \"testing\" a dataset refer to how a model learns from data and then has its performance evaluated. The training dataset is used to teach the model to recognize patterns and make predictions, while the testing dataset is used to assess how well the trained model generalizes to new, unseen data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "-;sklearn.preprocessing is a module in the scikit-learn library that provides functions and classes to transform raw data into a suitable format for machine learning algorithms. This module includes techniques for scaling, normalizing, encoding, and imputing data, which are crucial steps in preparing data for model training and improving model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "9. What is a Test set?\n",
        "-;In machine learning and data science, a test set is a portion of the dataset used to evaluate the performance of a model that has been trained on a separate dataset, called the training set. It's crucial for understanding how well the model generalizes to unseen data and for comparing the performance of different models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "-;In Python, data is typically split for model fitting by using the train_test_split function from the sklearn library. This function divides the dataset into two subsets: a training set for model training and a testing set for evaluating the model's performance on unseen data.\n",
        "When approaching a machine learning problem, a systematic approach is crucial. Here's a breakdown:\n",
        "Train Test Validation Split: How To & Best Practices [2024]\n",
        "1. Data Collection: Gather the necessary data from various sources, ensuring it's relevant and sufficient for the problem.\n",
        "2. Data Preparation: Clean and preprocess the data, handling missing values, outliers, and transforming data into a suitable format for modeling.\n",
        "All About Train Test Split - Shiksha Online\n",
        "3. Data Splitting: Divide the data into training, validation, and testing sets.\n",
        "4. Model Selection: Choose an appropriate machine learning algorithm based on the problem type (classification, regression, etc.) and the characteristics of the data.\n",
        "5. Training: Train the selected model using the training data.\n",
        "6. Evaluation: Evaluate the model's performance using the validation set and/or the testing set.\n",
        "7. Tuning and Refinement: Adjust model parameters and hyperparameters based on the evaluation results, potentially re-training the model for improved performance.\n",
        "8. Prediction: Use the trained model to make predictions on new, unseen data.\n",
        "\n",
        "To effectively approach a machine learning problem, follow a systematic process: first, clearly define the problem and its goal, then gather and prepare the necessary data, choose an appropriate machine learning model, train the model, evaluate its performance, and finally, iterate and refine the model based on the evaluation results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "-; Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons. It allows you to identify patterns, outliers, and potential issues in the data that could affect model performance. EDA helps you understand the data better, making it easier to choose the right model and preprocess the data effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "12. What is correlation\n",
        "-;In data science, correlation is a statistical measure that quantifies the relationship between two variables, indicating the strength and direction of their connection. It helps understand how two variables move together, whether they both increase or decrease, or one increases while the other decreases. Correlation does not imply causation; it simply describes the association between variables.\n",
        "\n",
        "\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "-;A negative correlation, also known as an inverse correlation, means that as one variable increases, the other variable decreases, and vice versa. This is a relationship where the variables move in opposite directions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "-;To find the correlation between variables in Python, several libraries can be used, including Pandas, NumPy, and SciPy. The Pandas library is commonly used for data manipulation, while NumPy and SciPy are used for mathematical operations. Visualizations can be done using Matplotlib and Seaborn.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "-; Causation means one event directly results from another event, while correlation simply means there is a relationship between two events. A correlation doesn't necessarily mean one event causes the other, as they may both be influenced by a third variable or be coincidental.\n",
        "Example:\n",
        "Causation:\n",
        "If you press a button, the light turns on. The action of pressing the button directly causes the light to turn on.\n",
        "Correlation:\n",
        "Eating ice cream and shark attacks are correlated because they both tend to happen more often in the summer. However, eating ice cream does not cause shark attacks. The correlation is due to a third factor, warmer weather, which leads to more people swimming in the ocean and consuming ice cream.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with eg\n",
        "-; An optimizer is an algorithm used to iteratively minimize a loss function during machine learning model training. It adjusts model parameters (like weights) to find the optimal values that minimize the difference between predicted and actual outputs. Common types include Gradient Descent, Stochastic Gradient Descent (SGD), and adaptive optimizers like Adam.\n",
        "\n",
        "Types of Optimizers:\n",
        "Gradient Descent:\n",
        "This is a fundamental optimization algorithm that iteratively adjusts parameters to move towards the minimum of the loss function. It calculates the gradient (direction and rate of change) of the loss function and updates parameters in the opposite direction.\n",
        "Example: Consider a linear regression model where the loss function is Mean Squared Error. Gradient Descent would adjust the model's coefficients (weights) based on the gradient of the loss function, aiming to minimize the error.\n",
        "Stochastic Gradient Descent (SGD):\n",
        "Unlike Gradient Descent, which uses the average gradient over the entire dataset, SGD uses the gradient calculated from a single data point (or a small batch of data points) at each iteration.\n",
        "Example: In training a neural network on a large dataset, SGD might update the weights after processing just one image at a time, making it faster and more efficient than Gradient Descent for large datasets.\n",
        "Adaptive Optimizers:\n",
        "These optimizers adapt the learning rate for each parameter based on its gradient and history, which can improve convergence speed and stability. Examples include Adam and AdaGrad.\n",
        "Adam (Adaptive Moment Estimation): Adam combines momentum and a running average of squared gradients, allowing for efficient exploration of the loss landscape and often leading to faster convergence than SGD.\n",
        "Adagrad (Adaptive Gradient Descent): Adagrad adapts the learning rate based on the magnitude of the gradients, giving smaller learning rates to parameters with frequent updates and larger learning rates to infrequent parameters.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "-;sklearn.linear_model is a module in the scikit-learn (sklearn) library in Python that implements various linear models for regression and classification tasks. Linear models predict the target variable using a linear function of the input features. These models are widely used in machine learning due to their simplicity, interpretability, and efficiency.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "-; The model.fit() function in machine learning frameworks like TensorFlow and scikit-learn is used to train a model on a given dataset. It adjusts the model's internal parameters to minimize the difference between the predicted outputs and the actual outputs. This process is also known as model fitting.\n",
        "The required arguments for model.fit() are typically the training data (features) and the corresponding target values (labels). Depending on the framework and the specific model, additional arguments may be required or optional, such as:\n",
        "epochs: The number of times the entire training dataset is passed through the model during training.\n",
        "batch_size: The number of samples processed at a time during training.\n",
        "validation_data: Data used to evaluate the model's performance during training, helping to prevent overfitting.\n",
        "callbacks: Functions that can be applied at different stages of training, such as saving the model or adjusting the learning rate.\n",
        "verbose: Controls the amount of output displayed during training.\n",
        "shuffle: Whether to shuffle the training data before each epoch.\n",
        "loss: Function to measure how well the model is performing on the training data.\n",
        "optimizer: Algorithm to update the model's weights during training.\n",
        "metrics: List of metrics to evaluate the model's performance during training and testing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "-;model.predict() â€“ A model can be created and fitted with trained data, and used to make a prediction: yhat = model.predict(X)\n",
        "model. predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics.It takes a single argument, typically a NumPy array (X_new), representing the new data points you want to predict. The function then returns the predicted labels (for classification) or values (for regression) for each of the input data points.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "-;25 Categorical Variable Examples (2025)In statistics, continuous variables represent numerical values that can take any value within a specified range, while categorical variables represent non-numerical data grouped into categories or groups. Continuous variables can be measured and have an infinite number of possible values, while categorical variables are qualitative and limited to a specific set of labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "-;Feature scaling in machine learning is the process of normalizing or standardizing the range of independent variables or features in a dataset. It helps ensure that all features contribute equally to the learning process, preventing features with larger scales from dominating the model's performance.\n",
        "How it helps in Machine Learning:\n",
        "Improved Algorithm Performance:\n",
        "Many algorithms, like K-Nearest Neighbors, Support Vector Machines, and Neural Networks, are sensitive to the scale of features. Scaling these features before training these algorithms can significantly improve accuracy and convergence speed.\n",
        "Equal Weighting:\n",
        "Without scaling, features with larger values (e.g., income vs. age) can disproportionately influence the model, leading to biased results. Scaling ensures that all features have a similar contribution, preventing dominance by any single feature.\n",
        "Model Stability:\n",
        "Scaled features can make models more stable and less prone to overfitting, especially when dealing with high-dimensional data.\n",
        "Faster Convergence:\n",
        "In gradient descent optimization, scaling features can significantly speed up the convergence of the learning process, as the search space is more evenly distributed.\n",
        "Improved Interpretability:\n",
        "Scaled features can make it easier to compare the relative importance of different features in the model's decision-making process, particularly in linear regression and tree-based models.\n",
        "Efficient Data Storage and Processing:\n",
        "In data lakehouses, scaling data can lead to data compression and more efficient storage and processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "-;Data scaling in Python involves transforming numerical features to a similar scale. This is crucial for machine learning algorithms sensitive to feature magnitude. Common scaling techniques include:\n",
        "1. Min-Max Scaling (Normalization)\n",
        "This method scales data to a range between 0 and 1. It is useful when the data distribution is not Gaussian or when the range is important.\n",
        "Python\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 100], [20, 200], [30, 300]])\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n",
        "2. Standardization (Z-score scaling)\n",
        "Standardization transforms data to have a mean of 0 and a standard deviation of 1. It is less sensitive to outliers compared to Min-Max scaling and is suitable for algorithms assuming a Gaussian distribution.\n",
        "Python\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 100], [20, 200], [30, 300]])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n",
        "3. Robust Scaling\n",
        "This technique scales data using the median and interquartile range (IQR), making it robust to outliers.\n",
        "Python\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 100], [20, 200], [1000, 300]])\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n",
        "4. Max Absolute Scaling\n",
        "This method scales data to the range [-1, 1] by dividing each value by the maximum absolute value in the feature.\n",
        "Python\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[-10, 100], [20, -200], [30, 300]])\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n",
        "It is important to note that the scaler should be fit on the training data and then applied to both the training and testing data to prevent data leakage.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "-;The sklearn.preprocessing module in scikit-learn provides functions and classes to transform raw data into a format suitable for machine learning algorithms. It encompasses various techniques, including:\n",
        "Scaling:\n",
        "Adjusts the range of feature values.\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "MinMaxScaler: Scales features to a specified range, often between 0 and 1.\n",
        "RobustScaler: Scales features using median and interquartile range, making it robust to outliers.\n",
        "Normalizer: Normalizes samples individually to unit norm.\n",
        "Encoding Categorical Features:\n",
        "Converts categorical data into numerical representations.\n",
        "OneHotEncoder: Creates binary columns for each category.\n",
        "OrdinalEncoder: Assigns numerical values to categories based on their order.\n",
        "Imputation:\n",
        "Fills in missing values.\n",
        "SimpleImputer: Replaces missing values with a specified strategy (e.g., mean, median, or most frequent).\n",
        "Polynomial Features:\n",
        "Generates polynomial combinations of features.\n",
        "PolynomialFeatures: Creates new features consisting of all polynomial combinations of the original features with degree less than or equal to the specified degree.\n",
        "Custom Transformers:\n",
        "Allows users to create their own preprocessing steps.\n",
        "FunctionTransformer: Constructs a transformer from an arbitrary callable.\n",
        "Preprocessing is a crucial step in the machine learning workflow as it can significantly impact model performance and efficiency.\n",
        "\n",
        "\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "-;To split data for model fitting in Python, the train_test_split function from the sklearn.model_selection module is commonly used. This function divides the dataset into training and testing subsets, which is crucial for evaluating the model's performance on unseen data.\n",
        "Here's how to use train_test_split:\n",
        "Python\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Sample data (replace with your actual data)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
        "y = np.array([0, 1, 0, 1, 0])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting arrays\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "In this example, test_size=0.2 indicates that 20% of the data will be used for testing, while the remaining 80% will be used for training. The random_state parameter ensures reproducibility by fixing the random seed.\n",
        "It's also possible to split the data into three sets: training, validation, and testing. The validation set is used for hyperparameter tuning and model selection, while the test set is used for final evaluation. This can be achieved by splitting the data twice:\n",
        "Python\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "In this case, 70% of the data is used for training, 15% for validation, and 15% for testing.\n",
        "For classification problems with imbalanced classes, stratified splitting can be used to ensure that each split has the same proportion of classes as the original dataset. This can be done by setting the stratify parameter in train_test_split to the target variable:\n",
        "Python\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "Another approach is K-fold cross-validation, where the data is divided into K folds, and the model is trained and evaluated K times, each time using a different fold as the test set and the remaining folds as the training set. This technique provides a more robust estimate of model performance.\n",
        "\n",
        "\n",
        "25. Explain data encoding?\n",
        "-;Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing. It involves transforming information into a specific code or format, ensuring it can be read and interpreted by a computer or other system. This process is crucial for various applications, including ensuring data integrity, security, and compatibility between different systems."
      ],
      "metadata": {
        "id": "YXoFpsnhUcnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOKVbAeTULJJ"
      },
      "outputs": [],
      "source": []
    }
  ]
}